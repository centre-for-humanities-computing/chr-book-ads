{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ucloud/.local/lib/python3.12/site-packages (3.6.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /home/ucloud/.local/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/ucloud/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: filelock in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/ucloud/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ucloud/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ucloud/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ucloud/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 scikit-learn-1.7.0 scipy-1.16.0 seaborn-0.13.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets matplotlib pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = ['lf_all', 'aal_all', 'od_all_clean', 'thi_all', 'vib_all']\n",
    "newspaper_aar = ['aar_all']\n",
    "newspaper_extra = ['aar_ext', 'rib_all', 'sla_all']\n",
    "path_root = '../../../ROOT/DATA/NEWSPAPERS/article_embs/embeddings_e5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: finding the best classifier, predicting unlabeled data for manual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'date', 'embedding', 'n_chunks_orig', 'clean_category',\n",
       "       'nøgle', 'text', 'category', 'article_length', 'characters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(f'{path_root}{newspapers[0]}')\n",
    "df = ds.to_pandas()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85489, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85092, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of each embedding\n",
    "df['embedding_shape'] = df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = df['embedding_shape'].max()[0]\n",
    "df = df[df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "Bekjendtgjørelser    35817\n",
       "Jndenlandsk          25184\n",
       "Udenlandsk           24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1427/1606508774.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Define the number of samples per class (adjust based on dataset size)\n",
    "n_samples_per_class = 24000  # Change as needed\n",
    "\n",
    "# Create a balanced dataset by sampling an equal number of instances per class\n",
    "df_balanced = df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), n_samples_per_class), random_state=42)\n",
    ")\n",
    "\n",
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    df_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_balanced['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.91      0.94      0.93      4800\n",
      "      Jndenlandsk       0.84      0.85      0.84      4800\n",
      "       Udenlandsk       0.91      0.88      0.89      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.92      0.95      0.94      4800\n",
      "      Jndenlandsk       0.85      0.85      0.85      4800\n",
      "       Udenlandsk       0.90      0.87      0.88      4800\n",
      "\n",
      "         accuracy                           0.89     14400\n",
      "        macro avg       0.89      0.89      0.89     14400\n",
      "     weighted avg       0.89      0.89      0.89     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on TF-IDF features\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.94      0.96      0.95      4800\n",
      "      Jndenlandsk       0.88      0.86      0.87      4800\n",
      "       Udenlandsk       0.90      0.91      0.91      4800\n",
      "\n",
      "         accuracy                           0.91     14400\n",
      "        macro avg       0.91      0.91      0.91     14400\n",
      "     weighted avg       0.91      0.91      0.91     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit on training data and transform both train and test sets\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['clean_category'].values\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_tfidf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the TF-IDF features\n",
    "print(f'Train classifier on TF-IDF features')\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_tfidf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599929, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1449/2593171623.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Create a balanced sample for testing and manually evaluating\n",
    "df_sampled = final_df.groupby('newspaper', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new unlabeled data using the trained TF-IDF vectorizer\n",
    "X_test_tfidf = vectorizer.transform(df_sampled['text'])\n",
    "\n",
    "# Predict categories for the new data based on tf_idf\n",
    "df_sampled['predicted_category_tf_idf'] = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "X_test_embs = np.vstack(df_sampled['embedding'].values)\n",
    "\n",
    "# Predict categories for the new data based on embeddings\n",
    "df_sampled['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n",
    "\n",
    "# Save results\n",
    "df_sampled[['article_id', 'date', 'predicted_category_tf_idf', 'predicted_category_embs', 'category', 'text']].to_csv('../results/predicted_sample_tfidf_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disagree = df_sampled[df_sampled['predicted_category_embs'] != df_sampled['predicted_category_tf_idf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 14)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disagree.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: using the best classifier to predict all unlabeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final_df with all unlabeled articles\n",
    "dfs = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ds = Dataset.load_from_disk(f'{path_root}{newspapers[i]}')\n",
    "    df_np = ds.to_pandas()\n",
    "    dfs.append(df_np)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Check the shape of each embedding\n",
    "final_df['embedding_shape'] = final_df['embedding'].apply(lambda x: np.array(x).shape)\n",
    "expected_dim = final_df['embedding_shape'].max()[0]\n",
    "final_df = final_df[final_df['embedding'].apply(lambda x: np.array(x).shape == (expected_dim,))].copy()\n",
    "\n",
    "# Add column with name newspaper\n",
    "final_df['newspaper'] = final_df['article_id'].str.extract(r'^(.*?)_')\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_category\n",
       "-1                   599929\n",
       "Bekjendtgjørelser     35817\n",
       "Jndenlandsk           25184\n",
       "Udenlandsk            24091\n",
       "Name: clean_category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('clean_category')['clean_category'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_category_tf_idf</th>\n",
       "      <th>evaluation_tf_idf</th>\n",
       "      <th>predicted_category_embs</th>\n",
       "      <th>evaluation_embs</th>\n",
       "      <th>category</th>\n",
       "      <th>true_label</th>\n",
       "      <th>bog_teater</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55097</th>\n",
       "      <td>aal_055098</td>\n",
       "      <td>1826-08-17</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Kjøbenhavn.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Nærheden heraf er opkommet en Jordbrand, som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142457</th>\n",
       "      <td>aal_142458</td>\n",
       "      <td>1840-12-05</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>t</td>\n",
       "      <td>Bekjendtgjørelser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ved Auctionen, Torsdagen den 10de dennes, i St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70491</th>\n",
       "      <td>aal_070492</td>\n",
       "      <td>1829-03-04</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Blandinger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En Kok i Paris har taget Livet af sig, fordi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146810</th>\n",
       "      <td>aal_146811</td>\n",
       "      <td>1841-08-10</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Udenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Nyeste Post-Efterretninger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Det hollandske Blad, Tolk der Vryheid, med hvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155768</th>\n",
       "      <td>aal_155769</td>\n",
       "      <td>1842-12-07</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Jndenlandsk</td>\n",
       "      <td>t</td>\n",
       "      <td>Fædrelandet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J Sagen: Postholder Prahl, for, imod ProviHarm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        article_id        date predicted_category_tf_idf evaluation_tf_idf  \\\n",
       "55097   aal_055098  1826-08-17               Jndenlandsk                 t   \n",
       "142457  aal_142458  1840-12-05               Jndenlandsk                 t   \n",
       "70491   aal_070492  1829-03-04                Udenlandsk                 t   \n",
       "146810  aal_146811  1841-08-10                Udenlandsk                 t   \n",
       "155768  aal_155769  1842-12-07               Jndenlandsk                 t   \n",
       "\n",
       "       predicted_category_embs evaluation_embs                    category  \\\n",
       "55097              Jndenlandsk               t                 Kjøbenhavn.   \n",
       "142457       Bekjendtgjørelser               t           Bekjendtgjørelser   \n",
       "70491               Udenlandsk               t                  Blandinger   \n",
       "146810              Udenlandsk               t  Nyeste Post-Efterretninger   \n",
       "155768             Jndenlandsk               t                 Fædrelandet   \n",
       "\n",
       "       true_label bog_teater  \\\n",
       "55097         NaN        NaN   \n",
       "142457        NaN        NaN   \n",
       "70491         NaN        NaN   \n",
       "146810        NaN        NaN   \n",
       "155768        NaN        NaN   \n",
       "\n",
       "                                                     text  \n",
       "55097   J Nærheden heraf er opkommet en Jordbrand, som...  \n",
       "142457  Ved Auctionen, Torsdagen den 10de dennes, i St...  \n",
       "70491   En Kok i Paris har taget Livet af sig, fordi m...  \n",
       "146810  Det hollandske Blad, Tolk der Vryheid, med hvi...  \n",
       "155768  J Sagen: Postholder Prahl, for, imod ProviHarm...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_set = pd.read_csv('../results/predicted_sample_tfidf_embeddings(1).csv', index_col=0, sep=';')\n",
    "gold_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2376/3149177884.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "gold_set = gold_set.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "gold_set[\"true_label\"] = gold_set.apply(\n",
    "    lambda row: row[\"predicted_category_tf_idf\"] if row[\"evaluation_tf_idf\"] == \"t\" else\n",
    "                (row[\"predicted_category_embs\"] if row[\"evaluation_embs\"] == \"t\" else row[\"true_label\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with df_gold_set on 'article_id' to get updated values\n",
    "final_df = final_df.merge(gold_set[['article_id', 'true_label']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'true_label' where available\n",
    "final_df['clean_category'] = final_df['true_label'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['true_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['label_type'] = final_df['clean_category'].apply(lambda x: 'predicted' if x == -1 else 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85882, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = final_df[final_df['label_type'] == 'gold']\n",
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2376/1950500359.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1990, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the subset where 'newspaper' == 'lol'\n",
    "lol_df = training_df[training_df['newspaper'] == 'lol']\n",
    "\n",
    "# Sample 200 random examples for each value of 'clean_category'\n",
    "lol_balanced = lol_df.groupby('clean_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=400, replace=True) if len(x) >= 400 else x\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Keep the other newspapers as they are\n",
    "other_newspapers_df = training_df[training_df['newspaper'] != 'lol']\n",
    "\n",
    "# Combine the balanced 'lol' subset with the other newspapers\n",
    "filtered_df = pd.concat([lol_balanced, other_newspapers_df], ignore_index=True)\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the balanced data into train and test sets with stratification\n",
    "train_df, test_df = train_test_split(\n",
    "    filtered_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=filtered_df['clean_category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier on embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1288: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Bekjendtgjørelser       0.92      0.88      0.89       136\n",
      "      Jndenlandsk       0.75      0.82      0.78       136\n",
      "       Udenlandsk       0.87      0.82      0.84       126\n",
      "\n",
      "         accuracy                           0.84       398\n",
      "        macro avg       0.84      0.84      0.84       398\n",
      "     weighted avg       0.84      0.84      0.84       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training and test features/labels\n",
    "X_train = np.vstack(train_df['embedding'].values)\n",
    "y_train = train_df['clean_category'].values\n",
    "\n",
    "X_test = np.vstack(test_df['embedding'].values)\n",
    "y_test = test_df['clean_category'].values\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "clf_embs = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the classifier on the labeled training data\n",
    "print(f'Train classifier on embeddings')\n",
    "clf_embs.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "predictions = clf_embs.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599139, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = final_df[final_df['label_type'] == 'predicted']\n",
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269/2012251868.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)\n"
     ]
    }
   ],
   "source": [
    "X_test_embs = np.vstack(pred_df['embedding'].values)\n",
    "\n",
    "# Predict categories for the unlabeled articles based on embeddings\n",
    "pred_df['predicted_category_embs'] = clf_embs.predict(X_test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge final_df with pred_df on 'article_id' to get updated values\n",
    "final_df = final_df.merge(pred_df[['article_id', 'predicted_category_embs']], on='article_id', how='left')\n",
    "\n",
    "# Overwrite 'clean_category' with 'predicted_category_embs' where available\n",
    "final_df['clean_category'] = final_df['predicted_category_embs'].fillna(final_df['clean_category'])\n",
    "\n",
    "# Drop the temporary 'true_label' column\n",
    "final_df.drop(columns=['predicted_category_embs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685021, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
